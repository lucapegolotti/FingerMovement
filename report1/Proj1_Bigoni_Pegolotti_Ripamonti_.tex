\documentclass{article}


\usepackage[left=3cm,right=3cm,top=2cm,bottom=3cm]{geometry} % page settings
\usepackage{amsmath,amsfonts,amsthm,amssymb} % provides many mathematical environments & tools


\usepackage{tikz}
\usetikzlibrary{shapes,arrows,positioning}
\usetikzlibrary{mindmap,trees, backgrounds}
\usetikzlibrary{decorations.pathreplacing}
\usepackage{booktabs}
\usepackage{listings}

\lstset{
  basicstyle=\ttfamily,
  mathescape
}


\tikzset{block/.style={rectangle, draw, text width=10em, text centered, rounded corners,
 minimum width=3.5cm}}
 \tikzset{blockL/.style={rectangle, draw, text width=14em, text centered, rounded corners,
 minimum width=3.5cm}}
\tikzset{block_color/.style={rectangle, draw, fill=BurntOrange!65, text width=10em, text centered, rounded corners,
 minimum width=3.5cm}}
 \tikzset{line/.style={draw, -latex}}
 
 \definecolor{green_m}{rgb}{0.0, 0.66, 0.47}
 
 
\usepackage{adjustbox}

\usepackage{bigints}
\usepackage{hyperref}
\newcommand{\lver}{\left|}
\newcommand{\rver}{\right|}
\newcommand{\bsy}[1]{\boldsymbol{#1}}

\usepackage{hyperref}
\hypersetup{%
  colorlinks=false,% hyperlinks will be black
  linkbordercolor=black,% hyperlink borders will be red
  pdfborderstyle={/S/U/W 1}% border style will be underline of width 1pt
}


% hyperref
\usepackage[]{hyperref}

% color box
%\newcommand{\boxcolor}{orange}
\makeatletter
\renewcommand{\boxed}[2]{\textcolor{#2}{%
\tikz[baseline={([yshift=-1ex]current bounding box.center)}] \node [thick, rectangle, minimum width=1ex,rounded corners,draw] {\normalcolor\m@th$\displaystyle#1$};}}



 \makeatother
\begin{document}

\title{Deep Learning Mini-Project 1 Report \\ Prediction of Finger Movements from EEG Recordings}

\author{Group 78\footnote{As agreed with Dr. F. Fleuret, L. Pegolotti has collaborated with group 78 for Project 1 and with group 79, with M. Martin, for Project 2. C. Bigoni and N. Ripamonti have worked together on both projects.}\\ Caterina Bigoni, Luca Pegolotti \,and Nicol\`o Ripamonti}
\date{18 May 2018}
\maketitle
%\tableofcontents
%\newpage



% \small{\textsuperscript{a}EPFL-SB-MATH-MCSS, \textsuperscript{b}EPFL-SB-MATH-DIV}


\section{Introduction}
 % Quick intro on the problem
 The goal of this project is to design and train a Deep Neural Network to analyse EEG data. 
 In particular, we perform a two-class classification of EEG time signals collected from one patient to predict the direction of her upcoming finger movements (left vs. right). 
 The  dataset, part of the ``BCI Competition II'' \cite{bci_ii}, is composed of 416 recordings  sampled with a 1000 Hz frequency for 28 channels (i.e., electrodes placed at different locations on the skull).
The dataset is split in training and test datasets of 316 and 100 samples, respectively. 

% Organisation of the report
This report is organised as follows: we will briefly present and visualise the datasets in Section \ref{sec_visual}, where we introduce the suggested down-sampling, a possible  data augmentation strategy and consider sample shuffling before splitting into train and test sets (si o no..?). 
Then, in Section \ref{sec_allmodel}, we will present five different models we tried and the corresponding results obtained on the test set.
Of these, the one that performs best is selected for further tests in Section \ref{sec_themodel}. 
In particular, we present a smoothing pre-processing strategy to reduce noise in the signals and  an optimisation setup to select the hyper-parameters.  
Final conclusions and discussions are provided in Section \ref{sec_conclusion}. 

 % Visualisation remarks -
 \section{Data Visualisation}\label{sec_visual}
 Before rushing at adding extra layers or more neurons to increase the complexity of our model, we  take a look at the given data. 
 Figure \ref{fig_mean_1000hz_vs_downsampled} shows the average over all samples of the training and test datasets for three randomly chosen channels, i.e. channels 3, 10 and 25. 
The averages are shown separately for the two-classes, i.e. left and right, and for both the signal sampled at 1000 Hz (in red and blue) and its down-sampled signals (in black and magenta). 
We can observe that the down-sampled signals present a less noisy average with respect to the 1000 Hz signal. 
Moreover, we remark that  the means over the test and train dataset are very different. 
This behaviour is observed, although with different intensity, for all 28 channels. 
One possible explanation of this discrepancy lies in the fact that the averages were not performed on equally numerous datasets: 316 samples and 100 samples in the training and test datasets, respectively. 
Indeed, we can confirm that the average obtained from a subset of the training dataset of 100 samples is quite different from the average over 316 samples shown in Figure \ref{fig_mean_1000hz_vs_downsampled}. 
%
 \begin{figure}[h]
 \begin{center}
  \includegraphics[width=1\textwidth]{fig/fig4new_mean_downsampled_vs1000} 
  \caption{Comparison of training and test datasets averages per channel over 316 and 100 samples, respectively.  
  Three channels (i.e., 3, 10 and 25) are randomly chosen, but the results generalise to all. 
  Each plot shows the mean of the samples with output 0 (i.e., classified as ``right'') and those with output 1 (i.e., classified as ``left''). 
  The means of the signals sampled with 1000 Hz are plotted in red and blue, while those of the signals down-sampled to 100 Hz ar plotted in black and magenta. 
For some channels (e.g., channel 3) the down-sampled averages present a lower variability than those for the signals sampled at 1000 Hz. 
  In all cases, the averages of the training set are distinctively different from those of the test set. 
  \label{fig_mean_1000hz_vs_downsampled}}
  \end{center}
  \end{figure}
  %
 This behaviour can be justified if  samples have a high variance and Figure \ref{fig_fewsamples_vs_mean_downsampled},which shows the behaviour of three randomly chosen down-sampled samples for each class (i.e., left and right) for both the training and test dataset, confirmes it. 
 Immediately, we can see that both the training and test selected samples are all very noisy and distant from their corresponding mean.
  \begin{figure}[h]
 \begin{center}
  \includegraphics[width=1\textwidth]{fig/fig5new_fewsamples_mean_downsampled} 
  \caption{Plot of few randomly chosen samples from training and test datasets for channels 3, 10 and 25.
  Samples classified as 0 (i.e., ``right'') are plotted in red, while   samples classified as 1 (i.e., ``left'') are plotted in blue.
  The down-sampled averages corresponding to the training and test dataset are also shown in black.
   These plots confirm that the data have a high variability. 
  \label{fig_fewsamples_vs_mean_downsampled}}
  \end{center}
  \end{figure}
  
 Even if we understand that the average is not a sufficient meaningful quantity to describe a complex dataset, the high variability of data makes us believe that this apparently simple two-class 1D classification problem, will instead be quite challenging. 
  In order to reduce the discrepancies, we  normalise the training dataset using its mean and standard deviation for all channels. 
  The same values are used for the test dataset. 
  

Finally, driven by the results already published for this dataset, known as dataset VI in the BCI competition \cite{bci_ii}, and supported by other studies (see e.g. \cite{schirrmeister2017deep}), we propose a preprocessing strategy to reduce both the generalisation error and the size of the model needed to fit the training set.
Indeed, by reducing the noise in the data, i.e., by increasing the signal-to-noise ration, we aim at reducing the amount of variation that the model has to take into account \cite{goodfellow2016deep} and thus . 
A \emph{filtering preprocessing} technique is therefore applied to our  final model, as explained in Section \ref{sec_themodel}.
In particular, we use the 1D Savitzky-Golay filter \cite{savgol} to smooth the data without greatly distorting the signal. 
By a convolution, the filter fits successive subsets of adjacent data points in a low-degree polynomial using the linear least squares method. 
In our tests, we use subsets of 7 points to be fitted in a polynomial of order 3.
Figure \ref{fig_smoothig_savgol} shows ....TODO
% \begin{figure}[h]
% \begin{center}
%  \includegraphics[width=1\textwidth]{fig/fig6_smoothing} 
%  \caption{lala
%  \label{fig_smoothig_savgol}}
%  \end{center}
%  \end{figure}

 % TOADD ???  - mixed train+test mean? // train only 100 vs train 316 mean? 
 

 
 \section{Models we considered}\label{sec_allmodel}
 In this Section, we describe some of the models we tested on the EEG signals and summarize their performance in terms of errors on a validation dataset. Each of these models is tested against the standard down-sampled dataset and an \emph{augmented dataset}. The latter is obtained by preprocesing the dataset to reduce the generalisation error \cite{goodfellow2016deep}. In particular, we exploit the 1000 Hz dataset to extract not one, but 10  signals with frequency 100 Hz. 
In this way, our training dataset has the following dimensions: 3160 samples, 28 channels, 50 time-steps. 
\textcolor{red}{We considered the possibility of extending the data-augmentation strategy to the test dataset to further increase the dimension of our training dataset by including some samples from the test dataset after shuffling. 
However, we soon realised that this would imply solving an oversimplified problem being the 10 down-sampled signals all very alike. 
We therefore apply the data-augmentation strategy only to the training dataset.}
 
In the following, we provide a small description of each model we considered. The diagrams representing the graph structures show the dimensions of the input and output tensor of each module composing the networks except for the non-linear nodes, which do not modify the dimensionality of the inputs; $n$ is the number of samples of the input.
\begin{itemize}
\item Linear perceptron (M1): the simplest model one could devise for a classification problem is the linear perceptron:  given an input tensor $x$, the output is computed by the model as $y_{ij} = \sum_{k}x_{ik} A_{jk} + b_{j}$, where $A$ is the matrix of weights, and $b$ is the bias vector.
\item Simple convolutional linear network (M2):
\begin{figure}[h]
\centering
\includegraphics[width=\textwidth, clip=true,trim=100 300 295 230]{fig/conv1.pdf}
\end{figure}

\item Multichannel deep convolutional neural network + linear perceptron (M3):
\begin{figure}[h!]
\centering
\includegraphics[width=\textwidth, clip=true,trim=30 40 70 50]{fig/conv2.pdf}
\end{figure}

The convolutional layer in this model is such that that different masks are applied to each channel; at the implementation level, this is achieved by setting the \verb|groups| parameter equal to 28 in the constructor of the model. Convolutions of this kind are employed to study multichannel uni-dimensional time-series also in \cite{zheng2014time}.
\item Shallow convolutional neural network (M4):
\begin{figure}[h!]
\centering
\includegraphics[width=\textwidth, clip=true,trim=80 200 520 200]{fig/conv3.pdf}
\end{figure}

In this network, the first and the second convolutions operate in the dimension of the time samples and the channels respectively. We took inspiration from \cite{schirrmeister2017deep} for the design of this model.
\end{itemize}
The models are tested on the standard down-sampled and augmented datasets to evaluate their performance. We use the stochastic gradient descent for the optimization step, with bash size equal to 100 or to 1000 when the train set is composed by 316 or 3160 samples respectively. The learning rates are $10^{-1}$ for the linear perceptron and $10^{-3}$ for the other three models.

Table~\ref{tab_results} shows the performance of the four models in terms of error on a validation dataset (obtained by randomly choosing 10\% of the samples of the train dataset) after 1000 iterations of the stochastic gradient descent. Surprisingly, the linear perceptron (M1) performs better than (M2) and (M3), leading to a smaller average error over all the runs and also to a smaller variance of the result. In the non-agumented dataset, however, the model that ensures in our experience a smaller minimum error and a smaller error on average is the shallow convolutional neural network.

Augmenting the dataset leads to a slightly smaller error for M1, but to a considerable loss of accuracy for the other three models. It is likely that, with a different choice of parameters (size of convolutional and pool nodes, kernels \ldots), the advantage of using larger datasets could be exploited also for M2, M3 and M4. 

Since the shallow convolutional network (M4) showed the most promising reults in these test runs, in the following Section we decide to focus on optimizing this particular network. It is important to notice that our findings are greatly determined by the choices of parameters for each of the networks we considered, and it is possible that other combinations for M2 and M3 could lead to smaller errors.

 \begin{table}
 \begin{center}
    \begin{tabular}{ l l l l l l l l l}
\toprule
     $ $ & \multicolumn{4}{c}{non-augmented dataset} & \multicolumn{4}{c}{augmented dataset} \\
     \cmidrule(lr){2-5}
     \cmidrule(lr){6-9}
    Trial & M1 & M2 & M3 & M4 & M1 & M2 & M3 & M4  \\
\midrule
    1 & 27 & 34 & 32 & 32 & 23 & 37 & 36 & 35 \\
    2 & 27 & 29 & 33 & 25 & 24 & 32 & 36 & 37 \\
    3 & 29 & 28 & 30 & 24 & 24 & 33 & 35 & 34\\
    4 & 30 & 23 & 29 & 29 & 24 & 34 & 37 & 34\\
    5 & 26 & 28 & 30 & 33 & 26 & 30 & 34 & 33\\
    6 & 27 & 29 & 32 & 26 & 26 & 33 & 40 & 33\\
    7 & 26 & 29 & 33 & 29 & 25 & 31 & 34 & 32\\
    8 & 27 & 27 & 29 & 21 & 24 & 32 & 44 & 29\\
    9 & 29 & 31 & 30 & 29 & 24 & 32 & 39 & 37 \\
    10 & 26 & 32 & 31 & 22 & 24 & 32 & 36 & 35\\
    \midrule
    Best & 26 & 23 & 28 & 21 & 23 & 28 & 34 & 29 \\
    Average & 27.4 & 29.0 & 30.9 & 27.0 & 24.4 & 32.6 & 37.1 & 32.9 \\
\bottomrule
    \end{tabular}
        \caption{Validation errors for the linear perceptron (M1), the simple convolutional neural network (M2), the multichannel deep convolutional neural network combined with the linear perceptron (M3) and the shallow convolutional neural network (M4).}
\end{center}
\label{tab_results}
\end{table}

  \section{Model that works best}\label{sec_themodel}
 
 it doesnt overfit + graph
 
 
 -- Optimization with Hyperparameters
  
 \section{Conclusion}\label{sec_conclusion}
 
 
 % References
\bibliographystyle{abbrv}
\bibliography{mybib_dl} 
  
  
  
\end{document}