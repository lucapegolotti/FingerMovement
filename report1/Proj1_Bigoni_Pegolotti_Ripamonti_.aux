\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{bci_ii}
\citation{bci_ii}
\citation{schirrmeister2017deep}
\citation{goodfellow2016deep}
\citation{savgol}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Data Visualisation}{1}{section.2}}
\newlabel{sec_visual}{{2}{1}{Data Visualisation}{section.2}{}}
\citation{goodfellow2016deep}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Comparison of training and test datasets averages per channel over 316 and 100 samples, respectively. Three channels (i.e., 3, 10 and 25) are randomly chosen, but the results generalise to all. Each plot shows the mean of the samples with output 0 (i.e., classified as ``right'') and those with output 1 (i.e., classified as ``left''). For some channels (e.g., channel 3) the down-sampled averages present a lower variability than those sampled at higher resolution. In all cases, the averages of the training set are distinctively different from those of the test set. }}{2}{figure.1}}
\newlabel{fig_mean_1000hz_vs_downsampled}{{1}{2}{Comparison of training and test datasets averages per channel over 316 and 100 samples, respectively. Three channels (i.e., 3, 10 and 25) are randomly chosen, but the results generalise to all. Each plot shows the mean of the samples with output 0 (i.e., classified as ``right'') and those with output 1 (i.e., classified as ``left''). For some channels (e.g., channel 3) the down-sampled averages present a lower variability than those sampled at higher resolution. In all cases, the averages of the training set are distinctively different from those of the test set}{figure.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Considered models}{2}{section.3}}
\newlabel{sec_allmodel}{{3}{2}{Considered models}{section.3}{}}
\citation{zheng2014time}
\citation{schirrmeister2017deep}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Plot of few randomly chosen samples from training and test datasets for channels 3, 10 and 25. Samples classified as 0 (i.e., ``right'') are plotted in red, while samples classified as 1 (i.e., ``left'') are plotted in blue. }}{3}{figure.2}}
\newlabel{fig_fewsamples_vs_mean_downsampled}{{2}{3}{Plot of few randomly chosen samples from training and test datasets for channels 3, 10 and 25. Samples classified as 0 (i.e., ``right'') are plotted in red, while samples classified as 1 (i.e., ``left'') are plotted in blue}{figure.2}{}}
\citation{goodfellow2016deep}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Validation percentage errors for the linear perceptron (M1), the simple convolutional linear network (M2), the multichannel deep CNN combined with the linear perceptron (M3) and the shallow CNN (M4) using the down-sampeld set or the augmented one.}}{4}{table.1}}
\newlabel{tab_results}{{1}{4}{Validation percentage errors for the linear perceptron (M1), the simple convolutional linear network (M2), the multichannel deep CNN combined with the linear perceptron (M3) and the shallow CNN (M4) using the down-sampeld set or the augmented one}{table.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Further investigation of CNN model}{4}{section.4}}
\newlabel{sec_themodel}{{4}{4}{Further investigation of CNN model}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Overfitting}{4}{subsection.4.1}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Intervals for each parameter adopted in the hyper-parameter optimisation step. The reported intervals are the result of an initial educated guess, which has then be refined when we noticed that for some too large or too small values convergence was not obtained. }}{5}{table.2}}
\newlabel{tab_hyperparam_optim}{{2}{5}{Intervals for each parameter adopted in the hyper-parameter optimisation step. The reported intervals are the result of an initial educated guess, which has then be refined when we noticed that for some too large or too small values convergence was not obtained}{table.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Hyper-parameter optimisation}{5}{subsection.4.2}}
\bibstyle{abbrv}
\bibdata{mybib_dl}
\bibcite{bci_ii}{1}
\bibcite{savgol}{2}
\bibcite{goodfellow2016deep}{3}
\bibcite{schirrmeister2017deep}{4}
\bibcite{zheng2014time}{5}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{6}{section.5}}
\newlabel{sec_conclusion}{{5}{6}{Conclusion}{section.5}{}}
